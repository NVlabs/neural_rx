# SPDX-FileCopyrightText: Copyright (c) 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: LicenseRef-NvidiaProprietary
#
# NVIDIA CORPORATION, its affiliates and licensors retain all intellectual
# property and proprietary rights in and to this material, related
# documentation and any modifications thereto. Any use, reproduction,
# disclosure or distribution of this material and related documentation
# without an express license agreement from NVIDIA CORPORATION or
# its affiliates is strictly prohibited.

[global]
label = 'e2e_large' # all relevant files such as weights will use this label
ebno = True # activate rate-adjusted SNR

[system]
n_size_bwp = 4
num_rx_antennas = 4
mcs_index = [14]
mcs_table = 1
carrier_frequency = 2140000000.0
subcarrier_spacing = 30000.0
n_start_grid = 0
slot_number = 0
frame_number = 0
cyclic_prefix = 'normal'
precoding = 'codebook'
n_cell_id = 1
tpmi = 2
symbol_allocation = [0, 14]
num_antenna_ports = 2
# DMRS config such that it minimizes the number of DMRS positions
# we mask the remaining few DMRS positions at the receiver input
# Alternatively, one could write a custom PUSCHTransmitter to entireley remove
# any DMRS.
dmrs_mapping_type = "A"
dmrs_config_type = 2
dmrs_type_a_position = 2
dmrs_additional_position = 0
dmrs_length = 1
dmrs_nid = [[1, 1], [1, 1]]
n_scid = 1
num_cdm_groups_without_data = 1
verbose = False
dmrs_port_sets = [[0]]
n_rntis = [1, 1]
n_ids = [1, 1]

[baseline]
demapping_type = 'maxlog'
num_bp_iter = 20
cn_type = 'boxplus'
# For large num_prbs (>100), a low reduced complexity
# LMMSE estimator is used where LMMSE is only performed over the
# lmmse_num_prbs PRBS
lmmse_num_prbs = -1 # n_size_bwp must be multiple of this constant

[neural_receiver]
num_nrx_iter = 8 # defines number of cgnn_it stages
num_nrx_iter_eval = 8 # iterations used for evaluation; must be <= num_nrx_iter
d_s = 64 # feature space dimensions; effectively, defines num filter kernels (or neurons) for all components
num_units_init = [128, 128] # num filter kernels for input CNN (each list entry defines one layer)
# no aggregation due to single user
num_units_agg = [[128], [128], [128], [128],[128], [128], [128], [128]] # number of neurons of state aggregation MLP (each list entry defines one layer)
num_units_state = [[128, 128], [128, 128], [128, 128], [128, 128], [128, 128], [128, 128], [128, 128], [128, 128]] # num filter kernels for stage update CNN (each list entry defines one layer)
num_units_readout = [128]  # number of neurons of state aggregation MLP (each list entry defines one layer)
max_num_tx = 1 # e2e only works for 1 transmitter
min_num_tx = 1
initial_chest = None # no DMRS available
custom_constellation = True # activates trainable transmitter
mask_pilots = True # mask DMRS positions for e2e experiments

# quantization and other custom layer types
layer_type_dense = "dense"
layer_type_conv = "sepconv" # or "conv"
layer_type_readout = "dense"
nrx_dtype = tf.float32

[training]
# each entry of the training schedule denotes
# [number of SGD iterations, learning rate, batch_size, trainable_const.]
# second part finetunes the neural receiver for a fixed constellation
training_schedule = {
    "num_iter": [3e5, 9e6],
    "learning_rate": [0.001, 0.001],
    "batch_size": [128, 128],
    "train_tx": [True, False],
    "min_training_snr_db": [[3.5], [1.]], # only 1 UE, is Eb/No [dB] if ebno==True
    "max_training_snr_db": [[3.5], [7.]], # only 1 UE, is Eb/No [dB] if ebno==True
    "double_readout": [True, True], # use additional MSE loss on h_hat
    "apply_multiloss": [True, True],
    "weighting_double_readout": [0.02, 0.01]} # weighting between MSE & BCE loss
# Remark: training SNR for custom const should be close to waterfall region

num_iter_train_save = 1000
max_ut_velocity = 56.
min_ut_velocity = 0.
channel_norm = False
cfo_offset_ppm = 0.0 # randomly sampled in [-cfo_offset_ppm, cfo_offset_ppm]
# TDL
channel_type = 'TDL-C300' # hint: it may be beneficial to also train the receiver partially on UMi channels (after tx is already converged)
eval_ebno_db_arr = [4.0]
xla = True # Activate XLA for the training loop
tfrecord_filename = "na" # only relevant if training is done with a dataset

[evaluation]
# the following parameters are used during evaluation
snr_db_eval_min = -2
snr_db_eval_max = 7
snr_db_eval_stepsize = 1
max_ut_velocity_eval = 56
min_ut_velocity_eval = 56
cfo_offset_ppm_eval = 0.0
tfrecord_filename_eval = "na"
channel_type_eval = 'TDL-B100' # 1 User
channel_norm_eval = False
n_size_bwp_eval = 132
batch_size_eval = 20
batch_size_eval_small = 2 # for kbest
